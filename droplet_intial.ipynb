{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Install Packages ---\n",
        "!pip install -q \\\n",
        "    catboost \\\n",
        "    lightgbm \\\n",
        "    pytorch-tabnet \\\n",
        "    torch \\\n",
        "    scikit-learn-intelex \\\n",
        "    tqdm \\\n",
        "    pandas \\\n",
        "    openpyxl \\\n",
        "    xgboost \\\n",
        "    joblib\n",
        "\n",
        "print(\"All packages installed! Runtime will restart automatically if needed.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go5Y_fBFzTdx",
        "outputId": "cc8598a0-b329-4c7b-c90e-cc5ecd944385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.3/111.3 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hAll packages installed! Runtime will restart automatically if needed.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# train_model.py – FULL VERSION (Google Drive + Fixed Stacking)\n",
        "# ==============================================================\n",
        "\n",
        "# --- 1. Mount Google Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- 3. Imports & Safe Optional Models ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestRegressor,\n",
        "    GradientBoostingRegressor,\n",
        "    HistGradientBoostingRegressor,\n",
        "    StackingRegressor\n",
        ")\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Safe imports\n",
        "models_to_try = {}\n",
        "try:\n",
        "    from xgboost import XGBRegressor\n",
        "    models_to_try['xgb'] = XGBRegressor\n",
        "    print(\"XGBoost imported\")\n",
        "except: print(\"XGBoost not available\")\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostRegressor\n",
        "    models_to_try['cat'] = CatBoostRegressor\n",
        "    print(\"CatBoost imported\")\n",
        "except: print(\"CatBoost not available\")\n",
        "\n",
        "try:\n",
        "    from lightgbm import LGBMRegressor\n",
        "    models_to_try['lgb'] = LGBMRegressor\n",
        "    print(\"LightGBM imported\")\n",
        "except: print(\"LightGBM not available\")\n",
        "\n",
        "TABNET_AVAILABLE = False\n",
        "try:\n",
        "    from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "    import torch\n",
        "    TABNET_AVAILABLE = True\n",
        "    print(\"TabNet + PyTorch imported\")\n",
        "except: print(\"TabNet not available\")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- 4. Google Drive Paths ---\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/ML_Sessile_Drop\"\n",
        "os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
        "\n",
        "CHECKPOINT_DIR = os.path.join(DRIVE_ROOT, \"checkpoints\")\n",
        "MODEL_DIR = os.path.join(CHECKPOINT_DIR, \"models\")\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "LOG_FILE = os.path.join(DRIVE_ROOT, \"training_log.csv\")\n",
        "PREDICTIONS_FILE = os.path.join(DRIVE_ROOT, \"test_predictions.csv\")\n",
        "\n",
        "# Initialize log\n",
        "log_df = pd.DataFrame(columns=['timestamp','phase','model','param','mse','pct_error','note'])\n",
        "if os.path.exists(LOG_FILE):\n",
        "    log_df = pd.read_csv(LOG_FILE)\n",
        "\n",
        "def _fmt(v, fmt=\"{:.2e}\"):\n",
        "    return fmt.format(v) if v is not None and not np.isnan(v) else \"—\"\n",
        "\n",
        "def log_step(phase, model=\"\", param=\"\", mse=None, pct_error=None, note=\"\"):\n",
        "    global log_df\n",
        "    ts = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    row = pd.DataFrame([{\n",
        "        'timestamp': ts, 'phase': phase, 'model': model, 'param': param,\n",
        "        'mse': mse, 'pct_error': pct_error, 'note': note\n",
        "    }])\n",
        "    log_df = pd.concat([log_df, row], ignore_index=True)\n",
        "    log_df.to_csv(LOG_FILE, index=False)\n",
        "    print(f\"[{ts}] {phase} | {model} | {note} | MSE:{_fmt(mse)} | %Err:{_fmt(pct_error,'{:.2f}%')}\")\n",
        "\n",
        "# --- 5. Load Data ---\n",
        "print(\"\\nLoading data...\")\n",
        "DATA_PATH = \"/content/drive/MyDrive/Main_final.xlsx\"  # CHANGE IF NEEDED\n",
        "data = pd.read_excel(DATA_PATH)\n",
        "data = data.drop_duplicates().copy()\n",
        "data['Diameter_m'] = data['Diameter'] / 1000.0\n",
        "data['Bo'] = 1000.0 * 9.81 * (data['Diameter_m']**2) / 0.072\n",
        "log_step(\"load\", note=f\"{len(data)} rows, Bo ∈ [{data['Bo'].min():.3f},{data['Bo'].max():.3f}]\")\n",
        "\n",
        "# --- 6. Features & Target ---\n",
        "feat_cols = ['Bo', 'ContactAngle', 'x']\n",
        "X_raw = data[feat_cols]\n",
        "y = data['y']\n",
        "\n",
        "# --- 7. Preprocessing ---\n",
        "print(\"Applying polynomial features (degree=2)...\")\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly.fit_transform(X_raw)\n",
        "poly_names = poly.get_feature_names_out(feat_cols)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_poly)\n",
        "\n",
        "X_train, X_test, y_train, y_test, idx_tr, idx_te = train_test_split(\n",
        "    X_scaled, y, data.index, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Train: {len(X_train)} | Test: {len(X_test)}\")\n",
        "\n",
        "# Save preprocessing\n",
        "joblib.dump(scaler, os.path.join(DRIVE_ROOT, \"scaler.joblib\"))\n",
        "joblib.dump(poly, os.path.join(DRIVE_ROOT, \"poly.joblib\"))\n",
        "joblib.dump(feat_cols, os.path.join(DRIVE_ROOT, \"feature_cols.joblib\"))\n",
        "\n",
        "# --- 8. Safe % Error ---\n",
        "def pct_err(y_true, y_pred):\n",
        "    mask = y_true > 1e-6\n",
        "    if not mask.any(): return np.inf\n",
        "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "\n",
        "# --- 9. Train Models ---\n",
        "trained = {}\n",
        "\n",
        "# 1. Random Forest\n",
        "print(\"\\nTraining Random Forest...\")\n",
        "rf = RandomForestRegressor(n_estimators=300, max_features='sqrt', random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "trained['rf'] = rf\n",
        "joblib.dump(rf, os.path.join(MODEL_DIR, \"rf_model.joblib\"))\n",
        "log_step(\"fit\", \"RF\", note=\"fitted\")\n",
        "\n",
        "# 2. Gradient Boosting\n",
        "print(\"\\nTraining Gradient Boosting...\")\n",
        "gb = GradientBoostingRegressor(n_estimators=400, max_depth=5, learning_rate=0.05, random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "trained['gb'] = gb\n",
        "joblib.dump(gb, os.path.join(MODEL_DIR, \"gb_model.joblib\"))\n",
        "log_step(\"fit\", \"GB\", note=\"fitted\")\n",
        "\n",
        "# 3. HistGradientBoosting\n",
        "print(\"\\nTraining HistGradientBoosting...\")\n",
        "hgb = HistGradientBoostingRegressor(max_iter=600, max_depth=7, learning_rate=0.05, random_state=42)\n",
        "hgb.fit(X_train, y_train)\n",
        "trained['hgb'] = hgb\n",
        "joblib.dump(hgb, os.path.join(MODEL_DIR, \"hgb_model.joblib\"))\n",
        "log_step(\"fit\", \"HGB\", note=\"fitted\")\n",
        "\n",
        "# 4. XGBoost\n",
        "if 'xgb' in models_to_try:\n",
        "    print(\"\\nTraining XGBoost...\")\n",
        "    xgb = XGBRegressor(n_estimators=500, max_depth=6, learning_rate=0.05, n_jobs=-1, random_state=42)\n",
        "    xgb.fit(X_train, y_train)\n",
        "    trained['xgb'] = xgb\n",
        "    joblib.dump(xgb, os.path.join(MODEL_DIR, \"xgb_model.joblib\"))\n",
        "    log_step(\"fit\", \"XGB\", note=\"fitted\")\n",
        "\n",
        "# 5. CatBoost\n",
        "if 'cat' in models_to_try:\n",
        "    print(\"\\nTraining CatBoost...\")\n",
        "    cat = CatBoostRegressor(iterations=600, depth=6, learning_rate=0.05, verbose=0, random_seed=42)\n",
        "    cat.fit(X_train, y_train)\n",
        "    trained['cat'] = cat\n",
        "    joblib.dump(cat, os.path.join(MODEL_DIR, \"cat_model.joblib\"))\n",
        "    log_step(\"fit\", \"CAT\", note=\"fitted\")\n",
        "\n",
        "# 6. LightGBM\n",
        "if 'lgb' in models_to_try:\n",
        "    print(\"\\nTraining LightGBM...\")\n",
        "    lgb = LGBMRegressor(n_estimators=600, max_depth=7, learning_rate=0.05, n_jobs=-1, random_state=42)\n",
        "    lgb.fit(X_train, y_train)\n",
        "    trained['lgb'] = lgb\n",
        "    joblib.dump(lgb, os.path.join(MODEL_DIR, \"lgb_model.joblib\"))\n",
        "    log_step(\"fit\", \"LGB\", note=\"fitted\")\n",
        "\n",
        "# 7. Neural Network\n",
        "print(\"\\nTraining Neural Network...\")\n",
        "nn = MLPRegressor(hidden_layer_sizes=(128,64,32), max_iter=2000, early_stopping=True,\n",
        "                  validation_fraction=0.1, learning_rate='adaptive', random_state=42)\n",
        "nn.fit(X_train, y_train)\n",
        "trained['nn'] = nn\n",
        "joblib.dump(nn, os.path.join(MODEL_DIR, \"nn_model.joblib\"))\n",
        "log_step(\"fit\", \"NN\", note=\"fitted\")\n",
        "\n",
        "# 8. TabNet (standalone, not in stacking)\n",
        "if TABNET_AVAILABLE:\n",
        "    print(\"\\nTraining TabNet (standalone)...\")\n",
        "    tabnet = TabNetRegressor(n_d=64, n_a=64, n_steps=5, gamma=1.5, lambda_sparse=1e-4,\n",
        "                             optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-2), verbose=0)\n",
        "    tabnet.fit(X_train, y_train.values.reshape(-1,1), max_epochs=200, patience=50, batch_size=256)\n",
        "    trained['tabnet'] = tabnet\n",
        "    joblib.dump(tabnet, os.path.join(MODEL_DIR, \"tabnet_model.joblib\"))\n",
        "    log_step(\"fit\", \"TABNET\", note=\"fitted (standalone)\")\n",
        "\n",
        "# --- 10. Stacking (ONLY sklearn-compatible models) ---\n",
        "print(\"\\nTraining Stacking Ensemble (sklearn models only)...\")\n",
        "valid_base_models = []\n",
        "for name, model in trained.items():\n",
        "    if name in ['tabnet', 'stack']:\n",
        "        continue\n",
        "    if hasattr(model, 'fit') and hasattr(model, 'predict'):\n",
        "        valid_base_models.append((name, model))\n",
        "\n",
        "if len(valid_base_models) < 2:\n",
        "    print(\"Not enough valid models for stacking. Skipping.\")\n",
        "    log_step(\"fit\", \"STACK\", note=\"skipped\")\n",
        "else:\n",
        "    stack = StackingRegressor(\n",
        "        estimators=valid_base_models,\n",
        "        final_estimator=LinearRegression(),\n",
        "        cv=5,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    stack.fit(X_train, y_train)\n",
        "    trained['stack'] = stack\n",
        "    joblib.dump(stack, os.path.join(MODEL_DIR, \"stack_model.joblib\"))\n",
        "    names = ', '.join([n for n,_ in valid_base_models])\n",
        "    log_step(\"fit\", \"STACK\", note=f\"{len(valid_base_models)} models: {names}\")\n",
        "\n",
        "# --- 11. Evaluation ---\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_df = data.loc[idx_te, ['Bo','ContactAngle','x','y']].copy().reset_index(drop=True)\n",
        "test_df.rename(columns={'y':'y_true'}, inplace=True)\n",
        "\n",
        "results = {}\n",
        "for name, model in trained.items():\n",
        "    if name == 'tabnet':\n",
        "        pred = model.predict(X_test).ravel()\n",
        "    else:\n",
        "        pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, pred)\n",
        "    err = pct_err(y_test.values, pred)\n",
        "    results[name] = {'mse':mse, 'pct_error':err}\n",
        "    test_df[f'y_pred_{name}'] = pred\n",
        "    log_step(\"eval\", name.upper(), mse=mse, pct_error=err, note=\"TEST\")\n",
        "\n",
        "# Save predictions\n",
        "test_df.to_csv(PREDICTIONS_FILE, index=False)\n",
        "\n",
        "# Save feature importance\n",
        "for n in ['rf','gb','hgb','xgb','cat','lgb']:\n",
        "    if n in trained and hasattr(trained[n], 'feature_importances_'):\n",
        "        pd.Series(trained[n].feature_importances_, index=poly_names)\\\n",
        "          .to_csv(os.path.join(DRIVE_ROOT, f\"importance_{n}.csv\"))\n",
        "\n",
        "print(f\"\\nSAVED TO GOOGLE DRIVE:\")\n",
        "print(f\"   Folder: {DRIVE_ROOT}\")\n",
        "print(f\"   Models: {MODEL_DIR}\")\n",
        "print(f\"   Predictions: {PREDICTIONS_FILE}\")\n",
        "print(f\"   Log: {LOG_FILE}\")\n",
        "\n",
        "# --- 12. Final Ranking ---\n",
        "print(\"\\n=== FINAL RANKING ===\")\n",
        "rank = sorted(results.items(), key=lambda x: x[1]['pct_error'])\n",
        "for n, r in rank:\n",
        "    print(f\"   {n.upper():8} -> %Err: {r['pct_error']:.3f}% | MSE: {r['mse']:.2e}\")\n",
        "print(f\"\\nBEST MODEL: {rank[0][0].upper()} ({rank[0][1]['pct_error']:.3f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCMXPsjgzKrl",
        "outputId": "237d0c28-632c-47c3-dab4-7efa85aad608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "All packages installed! Runtime will restart automatically if needed.\n",
            "\n",
            "XGBoost imported\n",
            "CatBoost imported\n",
            "LightGBM imported\n",
            "TabNet + PyTorch imported\n",
            "\n",
            "Loading data...\n",
            "[10:14:21] load |  | 58976 rows, Bo ∈ [0.085,5.494] | MSE:— | %Err:—\n",
            "Applying polynomial features (degree=2)...\n",
            "Train: 47180 | Test: 11796\n",
            "\n",
            "Training Random Forest...\n",
            "[10:14:53] fit | RF | fitted | MSE:— | %Err:—\n",
            "\n",
            "Training Gradient Boosting...\n",
            "[10:16:07] fit | GB | fitted | MSE:— | %Err:—\n",
            "\n",
            "Training HistGradientBoosting...\n",
            "[10:16:07] fit | HGB | fitted | MSE:— | %Err:—\n",
            "\n",
            "Training XGBoost...\n",
            "[10:16:08] fit | XGB | fitted | MSE:— | %Err:—\n",
            "\n",
            "Training CatBoost...\n",
            "[10:16:11] fit | CAT | fitted | MSE:— | %Err:—\n",
            "\n",
            "Training LightGBM...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001011 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1203\n",
            "[LightGBM] [Info] Number of data points in the train set: 47180, number of used features: 9\n",
            "[LightGBM] [Info] Start training from score 0.001483\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[10:16:13] fit | LGB | fitted | MSE:— | %Err:—\n",
            "\n",
            "Training Neural Network...\n",
            "[10:16:27] fit | NN | fitted | MSE:— | %Err:—\n",
            "\n",
            "Training TabNet (standalone)...\n",
            "[10:38:27] fit | TABNET | fitted (standalone) | MSE:— | %Err:—\n",
            "\n",
            "Training Stacking Ensemble (sklearn models only)...\n",
            "[10:47:30] fit | STACK | 7 models: rf, gb, hgb, xgb, cat, lgb, nn | MSE:— | %Err:—\n",
            "\n",
            "Evaluating on test set...\n",
            "[10:47:32] eval | RF | TEST | MSE:1.50e-08 | %Err:24.43%\n",
            "[10:47:32] eval | GB | TEST | MSE:2.12e-08 | %Err:48.79%\n",
            "[10:47:32] eval | HGB | TEST | MSE:2.04e-07 | %Err:159.87%\n",
            "[10:47:32] eval | XGB | TEST | MSE:2.92e-08 | %Err:63.51%\n",
            "[10:47:32] eval | CAT | TEST | MSE:2.35e-08 | %Err:57.91%\n",
            "[10:47:32] eval | LGB | TEST | MSE:1.68e-08 | %Err:39.29%\n",
            "[10:47:32] eval | NN | TEST | MSE:2.30e-06 | %Err:263.20%\n",
            "[10:47:33] eval | TABNET | TEST | MSE:9.82e-07 | %Err:263.70%\n",
            "[10:47:35] eval | STACK | TEST | MSE:1.42e-08 | %Err:29.18%\n",
            "\n",
            "SAVED TO GOOGLE DRIVE:\n",
            "   Folder: /content/drive/MyDrive/ML_Sessile_Drop\n",
            "   Models: /content/drive/MyDrive/ML_Sessile_Drop/checkpoints/models\n",
            "   Predictions: /content/drive/MyDrive/ML_Sessile_Drop/test_predictions.csv\n",
            "   Log: /content/drive/MyDrive/ML_Sessile_Drop/training_log.csv\n",
            "\n",
            "=== FINAL RANKING ===\n",
            "   RF       -> %Err: 24.433% | MSE: 1.50e-08\n",
            "   STACK    -> %Err: 29.179% | MSE: 1.42e-08\n",
            "   LGB      -> %Err: 39.290% | MSE: 1.68e-08\n",
            "   GB       -> %Err: 48.795% | MSE: 2.12e-08\n",
            "   CAT      -> %Err: 57.914% | MSE: 2.35e-08\n",
            "   XGB      -> %Err: 63.506% | MSE: 2.92e-08\n",
            "   HGB      -> %Err: 159.873% | MSE: 2.04e-07\n",
            "   NN       -> %Err: 263.202% | MSE: 2.30e-06\n",
            "   TABNET   -> %Err: 263.702% | MSE: 9.82e-07\n",
            "\n",
            "BEST MODEL: RF (24.433%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# predict_model.py – SMART PLOT CHOOSER (DNS may be missing!)\n",
        "# ==============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd, numpy as np, matplotlib.pyplot as plt, joblib, os, logging\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.spatial import cKDTree\n",
        "from datetime import datetime\n",
        "\n",
        "# ─── PATHS & LOG ───\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/ML_Sessile_Drop\"\n",
        "LOG_FILE   = os.path.join(DRIVE_ROOT, \"prediction_log.txt\")\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(message)s',\n",
        "                    filename=LOG_FILE, filemode='a')\n",
        "logging.info(\"=== SMART PREDICTION START ===\")\n",
        "\n",
        "MODEL_DIR  = os.path.join(DRIVE_ROOT, \"checkpoints\", \"models\")\n",
        "DNS_PATH   = \"/content/drive/MyDrive/Main_final.xlsx\"\n",
        "OUT        = DRIVE_ROOT\n",
        "os.makedirs(OUT, exist_ok=True)\n",
        "\n",
        "Δρ, g, σ = 1000.0, 9.81, 0.072\n",
        "\n",
        "# ─── LOAD DNS ONCE ───\n",
        "df = pd.read_excel(DNS_PATH).drop_duplicates()\n",
        "df['Bo'] = Δρ * g * (df['Diameter']/1000)**2 / σ\n",
        "print(f\"Loaded {len(df):,} DNS points\")\n",
        "\n",
        "# ─── LIST MODELS ───\n",
        "models = {f.replace(\".joblib\",\"\").replace(\"_model\",\"\"): os.path.join(MODEL_DIR,f)\n",
        "          for f in os.listdir(MODEL_DIR) if f.endswith(\".joblib\")}\n",
        "for i,k in enumerate(models,1): print(f\" [{i}] {k}\")\n",
        "choice = input(\"\\nPick model → \").strip().lower()\n",
        "if choice not in models: raise ValueError(\"Model not found!\")\n",
        "model = joblib.load(models[choice])\n",
        "scaler, poly, feat_cols = [joblib.load(os.path.join(DRIVE_ROOT,f))\n",
        "                           for f in [\"scaler.joblib\",\"poly.joblib\",\"feature_cols.joblib\"]]\n",
        "logging.info(f\"Model: {choice}\")\n",
        "\n",
        "# ─── USER INPUT ───\n",
        "D = float(input(\"\\nDiameter (mm) → \"))\n",
        "θ = float(input(\"Contact angle (°) → \"))\n",
        "if D<=0 or not(0<=θ<=180): raise ValueError(\"Bad input!\")\n",
        "\n",
        "D_scaled = round(D * 2**(-1/3), 2)\n",
        "Bo = Δρ * g * (D_scaled/1000)**2 / σ\n",
        "print(f\"\\nScaled D = {D_scaled} mm → Bo = {Bo:.6f}\")\n",
        "\n",
        "# ─── FIND NEAREST DNS (or None) ───\n",
        "tree = cKDTree(df[['Bo','ContactAngle']].drop_duplicates())\n",
        "dist, idx = tree.query([Bo, θ], k=1)\n",
        "nearBo, nearCA = df[['Bo','ContactAngle']].drop_duplicates().iloc[idx].values\n",
        "mask = (abs(df['Bo']-nearBo)<1e-9) & (abs(df['ContactAngle']-nearCA)<1e-6)\n",
        "dns_profile = df[mask].sort_values('x')\n",
        "\n",
        "if len(dns_profile)==0:\n",
        "    print(\"DNS NOT FOUND → Will still plot ML drop!\")\n",
        "    dns_ok = False\n",
        "    x_dns, y_dns = np.array([]), np.array([])\n",
        "else:\n",
        "    dns_ok = True\n",
        "    x_dns, y_dns = dns_profile['x'].values, dns_profile['y'].values\n",
        "    print(f\"Nearest DNS: Bo={nearBo:.5f}, θ={nearCA}° (dist={dist:.3f})\")\n",
        "\n",
        "logging.info(f\"D={D}, Dsc={D_scaled}, Bo={Bo:.6f}, CA={θ}, DNS={'YES' if dns_ok else 'NO'}\")\n",
        "\n",
        "# ─── ML PREDICTION ───\n",
        "input_df = pd.DataFrame({c: [Bo if c=='Bo' else θ if c=='ContactAngle' else x_dns][0]\n",
        "                         for c in feat_cols})\n",
        "if len(x_dns)==0:  # fallback grid\n",
        "    x_dns = np.linspace(-0.6, 0.6, 200) * (D_scaled/2)\n",
        "input_df = pd.DataFrame({\n",
        "    feat_cols[0]: [Bo] * len(x_dns),\n",
        "    feat_cols[1]: [θ] * len(x_dns),\n",
        "    feat_cols[2]: x_dns\n",
        "})\n",
        "X = scaler.transform(poly.transform(input_df))\n",
        "y_ml = model.predict(X).ravel() if choice!='tabnet' else model.predict(X).ravel()\n",
        "\n",
        "# ─── CLEAN REPEATS ───\n",
        "def clean(x,y,max_rep=3):\n",
        "    fx,fy,c,p=[],[],0,None\n",
        "    for xi,yi in zip(x,y):\n",
        "        if yi==p: c+=1\n",
        "        else: c=0\n",
        "        if c<=max_rep: fx.append(xi); fy.append(yi)\n",
        "        p=yi\n",
        "    return np.array(fx),np.array(fy)\n",
        "x_ml,y_ml = clean(x_dns,y_ml)\n",
        "x_dns,y_dns = clean(x_dns,y_dns)\n",
        "\n",
        "# ─── MIRROR ───\n",
        "mirror = lambda x,y: (np.concatenate([x,x[::-1]]),\n",
        "                      np.concatenate([y,-y[::-1]]))\n",
        "\n",
        "# ─── SAVE EXCEL ───\n",
        "def save_xl(name,x,y):\n",
        "    pd.DataFrame({'x_mm':x*1000,'y_mm':y*1000}).to_excel(\n",
        "        f\"{OUT}/{name}_D{D_scaled}_θ{θ}.xlsx\", index=False)\n",
        "    print(f\"Excel → {name}\")\n",
        "\n",
        "save_xl(f\"ML_{choice}\", x_ml, y_ml)\n",
        "if dns_ok: save_xl(\"DNS_nearest\", x_dns, y_dns)\n",
        "\n",
        "# ─── PLOT ENGINE ───\n",
        "def plot(ax, x, y, label, col):\n",
        "    xu,yu = mirror(x,y)\n",
        "    ax.plot(xu*1000, yu*1000, 'o-', color=col, mfc='none', ms=3, label=label)\n",
        "\n",
        "def finish(fig,ax,title,fname):\n",
        "    ax.axhline(0,color='k',lw=2)\n",
        "    ax.set_xlabel('x (mm)'); ax.set_ylabel('y (mm)')\n",
        "    ax.set_title(title, fontsize=13)\n",
        "    ax.grid(True,ls='--',alpha=0.4)\n",
        "    ax.axis('equal'); ax.legend()\n",
        "    fig.savefig(f\"{OUT}/{fname}\", dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    print(f\"Plot → {fname}\")\n",
        "\n",
        "# ─── SMART MENU ───\n",
        "print(\"\\nWHAT DO YOU WANT TO SEE?\")\n",
        "print(\"  1 → ML only\")\n",
        "print(\"  2 → DNS only\" + (\"\" if dns_ok else \" (NOT AVAILABLE)\"))\n",
        "print(\"  3 → Both together\")\n",
        "opt = input(\"Choose 1 / 2 / 3 → \").strip()\n",
        "\n",
        "if opt==\"1\" or not dns_ok:                     # ALWAYS WORKS\n",
        "    fig,ax=plt.subplots(figsize=(8,6))\n",
        "    plot(ax, x_ml, y_ml, choice.upper(), 'red')\n",
        "    finish(fig,ax, f\"{choice.upper()} – D={D_scaled} mm, θ={θ}°\",\n",
        "           f\"ML_{choice}_D{D_scaled}_θ{θ}.png\")\n",
        "\n",
        "if opt==\"2\" and dns_ok:\n",
        "    fig,ax=plt.subplots(figsize=(8,6))\n",
        "    plot(ax, x_dns, y_dns, 'DNS', 'blue')\n",
        "    finish(fig,ax, f\"DNS (Bo={nearBo:.3f}, θ={nearCA}°)\", f\"DNS_D{D_scaled}_θ{θ}.png\")\n",
        "\n",
        "if opt==\"3\" and dns_ok:\n",
        "    fig,ax=plt.subplots(figsize=(9,6))\n",
        "    plot(ax, x_ml, y_ml, choice.upper(), 'red')\n",
        "    plot(ax, x_dns, y_dns, 'DNS', 'blue')\n",
        "    finish(fig,ax, f\"Comparison – D={D_scaled} mm, θ={θ}°\",\n",
        "           f\"COMPARE_D{D_scaled}_θ{θ}.png\")\n",
        "\n",
        "# ─── % ERROR (only if DNS exists) ───\n",
        "if dns_ok:\n",
        "    interp = interp1d(x_dns, y_dns, kind='linear', fill_value='extrapolate')\n",
        "    y_dns_i = interp(x_ml)\n",
        "    m = (y_dns_i>0) & (y_ml>0)\n",
        "    if m.sum():\n",
        "        err = np.mean(abs((y_dns_i[m]-y_ml[m])/y_dns_i[m]))*100\n",
        "        print(f\"\\n% Error = {err:.3f}%\")\n",
        "        logging.info(f\"%Error={err:.3f}%\")\n",
        "\n",
        "logging.info(\"=== END ===\")\n",
        "print(f\"\\nEVERYTHING SAVED → {OUT}\")"
      ],
      "metadata": {
        "id": "jZ4a44AHpmjC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}